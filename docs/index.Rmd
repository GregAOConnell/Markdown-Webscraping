---
title: "Webscrape HTML page"
author: "Greg"
date: "2024-04-21"
output: html_document
---

```{r setup, include=FALSE}
library(reticulate)
library(tidyverse)
library(udpipe)
library(ggwordcloud)
library(rvest)
library(xml2)
library(tidytext)
library(SnowballC)
library(quanteda)
library(text2vec)
library(LDAvis)


library(SnowballC)

```


## Israel Descriptive Stats, Word Count

```{r, echo = FALSE}
text <- read.csv('israelframe.csv')
# extract only articles that mention Netanyahu
netanyahutest<-text%>%
  filter(str_detect(bodytext, '\\bNetanyahu\\b'))
#Find tokens in the body texts provided
tokens<-netanyahutest%>%
  unnest_tokens(input = bodytext, token='words', output='word')
#get rid of stop words
tokens<-tokens%>%
  anti_join(stop_words, by=join_by(word))
#summarize and arrange the words by freq
tokens_counts<-tokens%>%
  group_by(word)%>%
  summarise(n = n())%>%
  arrange(-n)



#make a cute wordcloud
tokens_counts%>%
  arrange(-n)%>%
  slice_head(n =50)%>%
  ggplot(aes(label = word, size=n)) +
  geom_text_wordcloud() +
  theme_minimal()



#most common inflection for each word stem and get the full word
topwords<-tokens %>%
  mutate(stem = wordStem(word)) %>%
  group_by(stem)%>%
  mutate(n = n())%>%
  arrange(-n)%>%
  slice_head(n =1)%>%
  ungroup()%>%
  arrange(-n)


topwords%>%
  slice_head(n=15)%>%
  ggplot(aes(x=reorder(word,n), y=n)) +
  geom_bar(stat='identity', color='black', fill='pink') + labs(x="Most Common Words", Y="Frequency", title="Most common words in articles containing Netanyahu")+
  coord_flip()


udpipe_download_model("english")

english_model <- udpipe_load_model('english-ewt-ud-2.5-191206.udpipe')
parsed_text <- udpipe_annotate(object=english_model, x = text$bodytext[1:3], doc_id = text$url[1:3], trace=2)

parsed_israel <- data.frame(parsed_text)
```

##Descriptive Stats Ukraine

```{r, echo=FALSE}
#descriptitve stats and word count
text <- read.csv('ukraineframe.csv')
# extract only articles that mention Putin
putintest<-text%>%
  filter(str_detect(bodytext, '\\bPutin\\b'))
#Keep metadata but sort through articles with putin in them 
tokens<-putintest%>%
  unnest_tokens(input = bodytext, token='words', output='word')
#remove stopwords
tokens<-tokens%>%
  anti_join(stop_words, by=join_by(word))
#arrange in order of how they appear
tokens_counts<-tokens%>%
  group_by(word)%>%
  summarise(n = n())%>%
  arrange(-n)
#top 50 words cloud
tokens_counts%>%
  arrange(-n)%>%
  slice_head(n =50)%>%
  ggplot(aes(label = word, size=n)) +
  geom_text_wordcloud() +
  theme_minimal()
#top 15 words arranged in freq order
tokens %>%
  mutate(stem = wordStem(word)) %>%
  group_by(stem)%>%
  summarize(n = n())%>%
  arrange(-n)%>%
  slice_head(n=15)
#most common inflection for each word stem and get the full word
topwords<-tokens %>%
  mutate(stem = wordStem(word)) %>%
  group_by(stem)%>%
  mutate(n = n())%>%
  arrange(-n)%>%
  slice_head(n =1)%>%
  ungroup()%>%
  arrange(-n)

topwords%>%
  slice_head(n=15)%>%
  ggplot(aes(x=reorder(word,n), y=n)) +
  geom_bar(stat='identity', color='black', fill='pink') + labs(x="Most Common Words", Y="Frequency", title="Most common words in articles containing the word Putin")+
  coord_flip()


```

#Biden Sentiment Analysis
```{r}
text <- read.csv('israelframe.csv')

# extract only paragraphs that mention Important entities



bidentest<-text%>%
  filter(str_detect(bodytext, '\\bBiden\\b'))


#Find tokens in the body texts provided
tokens<-bidentest%>%
  unnest_tokens(input = bodytext, token='words', output='word')

#get rid of stop words
tokens<-tokens%>%
  anti_join(stop_words, by=join_by(word))

#summarize and arrange the words by freq
tokens_counts<-tokens%>%
  group_by(word)%>%
  summarise(n = n())%>%
  arrange(-n)

tokens_counts%>%
  arrange(-n)%>%
  
  slice_head(n=20)%>%
  
  ggplot(aes(x=reorder(word, n), y=n))+ 
  geom_bar(stat='identity')+
  coord_flip() +
  theme_bw()

tokens_counts%>%
  arrange(-n)%>%
  slice_head(n =50)%>%
  ggplot(aes(label = word, size=n)) +
  geom_text_wordcloud() +
  theme_minimal()

get_sentiments('bing')%>%
  head()


tone<-tokens%>% 
  #keep only tokens that are in the sentiment dictionary
  inner_join(get_sentiments('bing'))%>%
  group_by(url)%>%
  summarise(n = n(),
            positive= sum(sentiment == 'positive'),
            negative = sum(sentiment =='negative'),
            average_tone_is= (positive-negative)/n
  )


ggplot(tone, aes(x=average_tone_is)) + 
  geom_histogram(color='black', fill='white') +
  labs(title="Histogram of Articles about Israel mentioning Biden Average Sentiment", x="Average Tone of Paragraphs Mentioning Biden",
       y="Frequency")

library(SnowballC)

tokens %>%
  
  mutate(stem = wordStem(word)) %>%
  group_by(stem)%>%
  summarize(n = n())%>%
  arrange(-n)%>%
  slice_head(n=15)
#most common inflection for each word stem and get the full word
topwords<-tokens %>%
  mutate(stem = wordStem(word)) %>%
  group_by(stem)%>%
  mutate(n = n())%>%
  arrange(-n)%>%
  slice_head(n =1)%>%
  ungroup()%>%
  arrange(-n)


topwords%>%
  slice_head(n=15)%>%
  ggplot(aes(x=reorder(word,n), y=n)) +
  geom_bar(stat='identity', color='black', fill='pink') + labs(x="Most Common Words", Y="Frequency", title="Most common words in paragraphs about Biden related to Israel")+
  coord_flip()


udpipe_download_model("english")

english_model <- udpipe_load_model('english-ewt-ud-2.5-191206.udpipe')
parsed_text <- udpipe_annotate(object=english_model, x = text$bodytext[1:3], doc_id = text$url[1:3], trace=2)

parsed_israel <- data.frame(parsed_text)
```
#Biden in Ukraine Sentiment
```{r, echo=FALSE}
text <- read.csv('ukraineframe.csv')

# extract only paragraphs that mention Important entities



biden_uk_test<-text%>%
  filter(str_detect(bodytext, '\\bBiden\\b'))



tokens<-biden_uk_test%>%
  unnest_tokens(input = bodytext, token='words', output='word')


head(tokens$word)
head(stop_words, n=15)
tokens<-tokens%>%
  anti_join(stop_words, by=join_by(word))
tokens_counts<-tokens%>%
  group_by(word)%>%
  summarise(n = n())%>%
  arrange(-n)

tokens_counts%>%
  arrange(-n)%>%
  
  slice_head(n=20)%>%
  
  ggplot(aes(x=reorder(word, n), y=n))+ 
  geom_bar(stat='identity')+
  coord_flip() +
  theme_bw()

tokens_counts%>%
  arrange(-n)%>%
  slice_head(n =50)%>%
  ggplot(aes(label = word, size=n)) +
  geom_text_wordcloud() +
  theme_minimal()

get_sentiments('bing')%>%
  head()


tone<-tokens%>% 
  #keep only tokens that are in the sentiment dictionary
  inner_join(get_sentiments('bing'))%>%
  group_by(url)%>%
  summarise(n = n(),
            positive= sum(sentiment == 'positive'),
            negative = sum(sentiment =='negative'),
            average_tone_uk <-  (positive-negative)/n
  )


ggplot(tone, aes(x=tone$`average_tone_uk <- (positive - negative)/n`)) + 
  geom_histogram(color='black', fill='white') +
  labs(title="Histogram of Paragraphs mentioning Biden related to Ukraine Average Sentiment", x="Average Tone of Paragraphs Mentioning Putin",
       y="Frequency")


library(SnowballC)

tokens %>%
  
  mutate(stem = wordStem(word)) %>%
  group_by(stem)%>%
  summarize(n = n())%>%
  arrange(-n)%>%
  slice_head(n=15)
#most common inflection for each word stem and get the full word
topwords<-tokens %>%
  
  mutate(stem = wordStem(word)) %>%
  group_by(stem)%>%
  mutate(n = n())%>%
  arrange(-n)%>%
  slice_head(n =1)%>%
  ungroup()%>%
  arrange(-n)


topwords%>%
  slice_head(n=15)%>%
  ggplot(aes(x=reorder(word,n), y=n)) +
  geom_bar(stat='identity', color='black', fill='pink') + labs(x="Most Common Words", Y="Frequency", title="Most common words in paragraphs mentioning Biden related to Ukraine")+
  coord_flip()
?aes()
?geom_bar
udpipe_download_model("english")

english_model <- udpipe_load_model('english-ewt-ud-2.5-191206.udpipe')
parsed_text <- udpipe_annotate(object=english_model, x = text$bodytext[1:3], doc_id = text$url[1:3], trace=2)

parsed_ukraine <- data.frame(parsed_text)


```
